// 'tokenize' means splitting up strings,
//  a behavior of Lexer.
const tokenize = (string) => { /* TODO */ };
const parse = (tokens) => { /* TODO */ };
const evaluate = (ast) => { /* TODO */ };

evaluate(parse(tokenize("your lang's code"));
